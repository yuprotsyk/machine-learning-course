{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Машинне навчання</h1>\n",
    "<p>Ю.С. Процик. Курс лекцій</p>\n",
    "<div align=\"right\"><em>Cформовано на основі <a href=\"https://mlcourse.ai\">відкритого курсу</a> Юрія Кашницького</em></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "# <center>Тема 11. Бустінг\n",
    "## <center>Частина 2. Порівняння Xgboost і градієнтного бустінга Sklearn</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План\n",
    "\n",
    "1. [Основні параметри](#1.-Основні-параметри)\n",
    "2. [Порівняння алгоритмів за часом роботи](#2.-Порівняння-алгоритмів-за-часом-роботи)\n",
    "3. [Висновки](#3.-Висновки)\n",
    "4. [Корисні ресурси](#4.-Корисні-ресурси)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Основні параметри"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "### XGBoost\n",
    "\n",
    "Виділяють три групи параметрів:\n",
    "- Загальні параметри, що відповідають за базовий алгоритм для бустінга і розпаралелювання.\n",
    "- Параметри обраного базового алгоритму.\n",
    "- Параметри навчання, що відповідають за функцію втрат і метрику якості на валідації.\n",
    "\n",
    "**1. Загальні параметри:**\n",
    "- `booster` [default=gbtree] – тип базового алгоритму для бустінга: дерево рішень gbtree або лінійна модель gblinear.\n",
    "- `verbosity` [default=1] – повідомлення в процесі роботи алгоритму. Допустимі значення: 0 (silent), 1 (warning), 2 (info), 3 (debug).\n",
    "- `nthread` [default to maximum number of threads available if not set] – кількість потоків доступних для паралельної роботи xgboost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "**2. Параметри базового алгоритму:**\n",
    "\n",
    "**2.1. Дерево рішень:**\n",
    "- `eta` [default=0.3] – швидкість (темп) навчання, перед додаванням дерева в композицію воно множиться на eta. Використовується для запобігання перенавчання за рахунок \"скорочення\" ваг базових алгоритмів, роблячи модель більш консервативною. Чим менше eta, тим більше потрібно ітерацій `num_boost_round` для навчання моделі з хорошою якістю. Діапазон: [0, 1]\n",
    "- `gamma` [default=0] – мінімальне зниження значення функції втрат, необхідне для подальшого розбиття вершини дерева. Більші значення gamma > 0 призводять до більш консервативних моделей. Діапазон: [0, $\\infty$).\n",
    "- `max_depth` [default=6] – максимальна глибина дерева. Діапазон: [1, $\\infty$).\n",
    "- `min_child_weight` [default=1] – мінімальна необхідна (зважена) кількість прикладів в кожній вершині. Чим більша, тим більш консервативна кінцева модель. Діапазон: [0, $\\infty$).\n",
    "- `max_delta_step` [default=0] – зазвичай дорівнює нулю. Додатні значення використовуються при незбалансованих класах для прискорення збіжності. Діапазон [0, $\\infty$).\n",
    "- `subsample` [default=1] – частка вибірки, яка використовується для навчання кожного дерева. Якщо subsample < 1, то вибирається випадкова підвибірка, що допомагає в боротьбі з перенавчанням. Діапазон: (0, 1]\n",
    "- `colsample_bytree` [default=1] – частка ознак, яка використовується для навчання кожного дерева. Діапазон: (0, 1]\n",
    "- `lambda` [default=1] – коефіцієнт перед $L_2$-регуляризатором в функції втрат.\n",
    "- `alpha` [default=0] – коефіцієнт перед $L_1$-регуляризатором в функції втрат.\n",
    "\n",
    "**2.2. Лінійна модель:**\n",
    "- `lambda` [default=0] – коефіцієнт перед $L_2$-регуляризатором вектора ваг в функції втрат.\n",
    "- `alpha` [default=0] – коефіцієнт перед $L_1$-регуляризатором вектора ваг в функції втрат.\n",
    "- `lambda_bias` [default=0] – коефіцієнт перед $L_2$-регуляризатором зсуву (вільного члена) в функції втрат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "**3. Параметри задачі навчання:**\n",
    "- `objective` [default=reg:linear] – функція втрат, що використовується під час навчання:\n",
    "    - \"reg:linear\" – лінійна регресія.\n",
    "    - \"reg:logistic\" – логістична регресія.\n",
    "    - \"binary:logistic\" – логістична регресія для бінарної класифікації, на виході – ймовірність.\n",
    "    - \"binary:logitraw\" – те ж саме, але на виході – значення до його перетворення логістичною функцією.\n",
    "    - \"count:poisson\" – регресія Пуассона (використовується для оцінки числа якихось подій, злічена ознака), на виході – математичне сподівання розподілу Пуассона. В цьому випадку `max_delta_step` автоматично встановлюється рівним 0.7.\n",
    "    - \"multi:softmax\" – узагальнення логістичної регресії на багатокласовий випадок. При цьому потрібно задати параметр `num_class`.\n",
    "    - \"multi:softprob\" – те ж саме, але на виході - вектор розміру ndata * nclass, який можна перетворити в матрицю, що містить ймовірності віднесення даного об'єкта до даного класу.\n",
    "    - \"rank:pairwise\" – використовується для задач ранжування.\n",
    "- `base_score` [default=0.5] – ініціалізація значення моделі для всіх прикладів, глобальний зсув.\n",
    "- `eval_metric` [default according to objective] – метрика якості на валідаційній вибірці (за замовчуванням відповідає функції втрат: rmse – для регресії, error – для класифікації, mean average precision – для ранжування). Деякі з доступних метрик:\n",
    "    - \"rmse\": root mean square error.\n",
    "    - \"logloss\": мінус логарифм правдоподібності.\n",
    "    - \"error\": частка помилок для бінарної класифікації.\n",
    "    - \"merror\": те ж саме для багатокласової класифікації.\n",
    "    - \"mlogloss\": logloss для багатокласової класифікації.\n",
    "    - \"auc\": AUC.\n",
    "    - \"ndcg\": Normalized Discounted Cumulative Gain.\n",
    "    - \"map\": Mean average precision.\n",
    "- `seed` [default=0] – для відтворюваності \"випадковості\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "**Параметри в xgboost.train**:\n",
    "- `params` (dict) – параметри, описані вище.\n",
    "- `dtrain` (DMatrix) – навчальна вибірка.\n",
    "- `num_boost_round` (int) – кількість ітерацій бустінга.\n",
    "- `evals` (list) – список для оцінки якості під час навчання.\n",
    "- `obj` (function) – власна функція втрат.\n",
    "- `feval` (function) – власна функція для оцінки якості.\n",
    "- `maximize` (bool) – чи потрібно максимізувати `feval`.\n",
    "- `early_stopping_rounds` (int) – активує early stopping. Помилка на валідації повинна зменшуватися кожні `early_stopping_ rounds` ітерацій для продовження навчання. Список `evals` повинен бути непорожній. Повертається модель з останньої ітерації. Якщо відбулася рання зупинка, то модель буде містити поля: `bst.best_score` і `bst.best_iteration`.\n",
    "- `evals_result` (dict) – результати оцінки якості.\n",
    "- `verbose_eval` (bool) – вивід значення метрики якості на кожній ітерації бустінга.\n",
    "- `learning_rates` (list or function) – коефіцієнт швидкості навчання для кожної ітерації.\n",
    "- `xgb_model` (file name of stored xgb model or 'Booster' instance) – можливість продовжити навчання наявної моделі XGB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "## sklearn.ensemble.GradientBoostingClassifier\n",
    "- `loss` [default=\"deviance\"] – функція втрат, що оптимізується. Одна з {\"deviance\", \"exponential\"}. Перша відповідає логістичній регресії і повертає ймовірності, друга – AdaBoost.\n",
    "- `learning_rate` [default=0.1] – темп навчання, аналогічно `eta` для XGBoost.\n",
    "- `n_estimators` [default=100] – кількість ітерацій градієнтного бустінга.\n",
    "- `max_depth` [default=3] – аналогічно `max_depth` для XGBoost.\n",
    "- `min_samples_split` [default=2] – мінімальна кількість прикладів, необхідна для розгалуження в даній вершині, аналогічно `min_child_weight` для XGBoost.\n",
    "- `min_samples_leaf` [default=1] – мінімальна кількість прикладів в листку.\n",
    "- `min_weight_fraction_leaf` [default=0.0] – мінімальна зважена кількість прикладів в листку.\n",
    "- `subsample` [default=1.0] – аналогічно `subsample` для XGBoost.\n",
    "- `max_features` (int, float, string or None) [default=None] – кількість (або частка) ознак, що використовуються при розбитті вершини.\n",
    "    - \"auto\", тоді max_features=sqrt(n_features).\n",
    "    - \"sqrt\", тоді max_features=sqrt(n_features).\n",
    "    - \"log2\", тоді max_features=log2(n_features).\n",
    "    - None, тоді max_features=n_features.\n",
    "- `max_leaf_nodes` [default=None]\n",
    "- `init` (BaseEstimator or None) [default=None] – алгоритм для початкових прогнозів.\n",
    "- `verbose` [default=0] – аналогічно `silent` для XGBoost.\n",
    "- `warm_start` [default=False] – якщо True, використовується ансамбль з попереднього виклику fit, нові алгоритми додаються до нього, інакше будується новий алгоритм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "## 2. Порівняння алгоритмів за часом роботи\n",
    "\n",
    "Подивимося на час навчання класифікаторів XGBooster і GradientBoostingClassifier. Для цього будемо генерувати вибірку з 1000 об'єктів і 50 ознак за допомогою sklearn.datasets.make_classification і заміряти час навчання."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef500dbf13a548539c84daee1c76914c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "xgb_params1 = {'booster': 'gbtree', 'max_depth': 3, 'eta': 0.1, \n",
    "    'verbosity': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'nthread': 1}\n",
    "xgb_params2 = {'booster': 'gbtree', 'max_depth': 3, 'eta': 0.1, \n",
    "    'verbosity': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'nthread': 4}\n",
    "sklearn_params = {'n_estimators': 100, 'max_depth': 3}\n",
    "\n",
    "xgb_time1 = list()\n",
    "xgb_time2 = list()\n",
    "sklearn_time = list()\n",
    "\n",
    "n_runs = 50\n",
    "\n",
    "for i in tqdm(range(n_runs)):\n",
    "    # Generating dataset\n",
    "    X, y = make_classification(n_samples=1000, n_features=50, \n",
    "                               n_informative=20)\n",
    "    # Training XGBooster (nthread=1)\n",
    "    t = time.time()\n",
    "    bst = xgb.train(xgb_params1, xgb.DMatrix(X, label=y), \n",
    "                    num_boost_round=100)\n",
    "    elapsed = time.time() - t\n",
    "    xgb_time1.append(elapsed)\n",
    "    # Training XGBooster (nthread=4)\n",
    "    t = time.time()\n",
    "    bst = xgb.train(xgb_params2, xgb.DMatrix(X, label=y), \n",
    "                    num_boost_round=100)\n",
    "    elapsed = time.time() - t\n",
    "    xgb_time2.append(elapsed)\n",
    "    # Training GradientBoostingClassifier\n",
    "    t = time.time()\n",
    "    clf = GradientBoostingClassifier(**sklearn_params).fit(X, y)\n",
    "    elapsed = time.time() - t\n",
    "    sklearn_time.append(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib  inline\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "\n",
    "xgb_mean1 = sum(xgb_time1) / n_runs\n",
    "xgb_mean2 = sum(xgb_time2) / n_runs\n",
    "sklearn_mean = sum(sklearn_time) / n_runs\n",
    "\n",
    "xgb_min1 = min(xgb_time1)\n",
    "xgb_min2 = min(xgb_time2)\n",
    "sklearn_min = min(sklearn_time)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(xgb_time1, label='XGBooster (nthread = 1)', lw=2)\n",
    "plt.plot(xgb_time2, label='XGBooster (nthread = 4)', lw=2)\n",
    "plt.plot(sklearn_time, label='GradientBoostingClassifier', lw=2)\n",
    "plt.legend(loc='best')\n",
    "plt.text(1, (xgb_mean1 + sklearn_mean) / 2, \n",
    "         'XGBoost (nthread = 1): min time = %.2f, mean time = %.2f' % (xgb_min1, xgb_mean1) +\n",
    "         '\\n\\nXGBoost (nthread = 4): min time = %.2f, mean time = %.2f' % (xgb_min2, xgb_mean2)  +\n",
    "         '\\n\\nScikit-learn: min time = %.2f, mean time = %.2f' % (sklearn_min, sklearn_mean),\n",
    "            fontsize = 12)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Running time, sec')\n",
    "plt.title('XGBoost vs. GradientBoostingClassifier comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "## 3. Висновки\n",
    "\n",
    "Основні переваги XGBoost в порівнянні з sklearn.ensembles.GradientBoostingClassifier:\n",
    "- Крім дерев можливе використання лінійних моделей в якості базових класифікаторів.\n",
    "- Швидкість роботи.\n",
    "- Можливість розпаралелювання.\n",
    "- Значно більший вибір стандартних функцій втрат, а також можливість задавати свою функцію втрат.\n",
    "- Наявність регуляризаторів в кінцевій функції втрат і можливість задавати їх коефіцієнти, що дає ще один метод боротьби з перенавчанням, крім використання випадковості (`subsample`, `colsample_bytree`) і основних параметрів дерева рішень.\n",
    "- Вбудована обробка missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "## 4. Корисні ресурси\n",
    "- [XGBoost](https://xgboost.readthedocs.org/en/latest/parameter.html)\n",
    "- [GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "- [Порівняння](https://github.com/szilard/benchm-ml) різних бібліотек для машинного навчання, в тому числі sklearn і xgboost"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "2_1_7_regul.ipynb",
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "ru",
   "targetLang": "uk",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
