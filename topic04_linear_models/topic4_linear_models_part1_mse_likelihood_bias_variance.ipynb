{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Машинне навчання</h1>\n",
    "<p>Ю.С. Процик. Курс лекцій</p>\n",
    "<div align=\"right\"><em>Cформовано на основі <a href=\"https://mlcourse.ai\">відкритого курсу</a> Юрія Кашницького</em></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "# <center>Тема 4. Лінійні моделі класифікації та регресії</center>\n",
    "\n",
    "## <center>Частина 1. Лінійна регресія</center>\n",
    "\n",
    "## План\n",
    "\n",
    "1. [Метод найменших квадратів](#1.-Метод-найменших-квадратів)\n",
    "2. [Метод максимальної правдоподібності](#2.-Метод-максимальної-правдоподібності)\n",
    "3. [Розклад помилки на зсув та розкид (Bias-variance decomposition)](#3.-Розклад-помилки-на-зсув-та-розкид-(Bias-variance-decomposition))\n",
    "4. [Регуляризація лінійної регресії](#4.-Регуляризація-лінійної-регресії)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "## 1. Метод найменших квадратів\n",
    "\n",
    "Тему лінійних моделей розпочнемо з лінійної регресії. В першу чергу, необхідно задати модель залежності пояснюваної (залежної, цільової) змінної $y$ від факторів (ознак), що її пояснюють (незалежних змінних). Функція залежності буде лінійною: $y = w_0 + \\sum\\limits_{i=1}^m w_i x_i$. Якщо ми додамо фіктивну розмірність $x_0 = 1$ для кожного спостереження, тоді лінійну форму можна переписати більш компактно, записавши вільний член $w_0$ під суму: $y = \\sum\\limits_{i=0}^m w_i x_i = \\textbf{w}^{\\text{T}} \\textbf{x}$. Якщо розглядати матрицю спостереження-ознаки, в якій в рядках знаходяться приклади з набору даних, то нам необхідно додати зліва стовпець одиниць. Задамо модель наступним чином:\n",
    "\n",
    "$$\\large \\textbf y = \\textbf{X} \\textbf w + \\epsilon,$$\n",
    "\n",
    "де\n",
    "- $\\textbf{w}\\in \\mathbb{R}^{m+1}$ – вектор-стовпець параметрів моделі (в машинному навчанні ці параметри часто називають *вагами*);\n",
    "- $\\textbf X \\in \\mathbb{R}^{n \\times (m+1)}$ – матриця спостережень і ознак розмірності $n$ рядків на $m + 1$ стовпців\n",
    "(включаючи фіктивний стовпець одиниць зліва) з повним рангом за стовпцями: $\\text{rank}\\left(\\textbf{X}\\right) = m+1$;\n",
    "- $\\epsilon \\in \\mathbb{R}^n$ – випадковий вектор-стовпець, що відповідає випадковій, непрогнозованій помилці моделі (*помилка*, *шум*);\n",
    "- $\\textbf{y} \\in \\mathbb{R}^n$ – вектор стовпець – залежна (або *цільова*) змінна.\n",
    "\n",
    "Можемо виписати вираз для кожного конкретного спостереження\n",
    "\n",
    "$$\\large \n",
    "y_i = \\sum_{j=0}^m w_j X_{ij} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "Також на модель накладаються такі обмеження (умови Гаусса-Маркова, інакше це буде якась інша регресія, але точно не лінійна):\n",
    "- математичне сподівання випадкових помилок дорівнює нулю: $\\forall i: \\mathbb{E}\\left[\\epsilon_i\\right] = 0$;\n",
    "- дисперсія випадкових помилок однакова і скінченна, ця властивість називається [гомоскедастичністю](https://uk.wikipedia.org/wiki/%D0%93%D0%BE%D0%BC%D0%BE%D1%81%D0%BA%D0%B5%D0%B4%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%BD%D1%96%D1%81%D1%82%D1%8C): $\\forall i: \\text{Var}\\left(\\epsilon_i\\right) = \\sigma^2 < \\infty$;\n",
    "- випадкові помилки некорельовані: $\\forall i \\neq j: \\text{Cov}\\left(\\epsilon_i, \\epsilon_j\\right) = 0$.\n",
    "\n",
    "Оцінка $\\widehat{w}_i$ ваг $w_i$ називається *лінійною*, якщо\n",
    "\n",
    "$$\\large \\widehat{w}_i = \\omega_{1i}y_1 + \\omega_{2i}y_2 + \\cdots + \\omega_{1n}y_n,$$\n",
    "\n",
    "де $\\forall\\ k\\ \\omega_{ki}$ залежить тільки від спостережуваних даних $\\textbf{X}$ і майже напевно нелінійно. Так як розв'язком задачі пошуку оптимальних ваг буде саме лінійна оцінка, то і модель називається *лінійної регресією*. Дамо ще одне означення. Оцінка $\\widehat{w}_i$ називається *незміщеною*, якщо математичне сподівання оцінки дорівнює реальному, але невідомому значенню оцінюваного параметра:\n",
    "\n",
    "$$\\large \\mathbb{E}\\left[\\widehat{w}_i\\right] = w_i$$\n",
    "\n",
    "Один із способів обчислити значення параметрів моделі є **метод найменших квадратів** (МНК), який мінімізує середньоквадратичну помилку між реальним значенням залежної змінної і прогнозом, виданим моделлю:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}L\\left(\\textbf{X}, \\textbf{y}, \\textbf{w} \\right) &=& \\frac{1}{2n} \\sum\\limits_{i=1}^n \\left(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i\\right)^2 \\\\\n",
    "&=& \\frac{1}{2n} \\left\\| \\textbf{y} - \\textbf{X} \\textbf{w} \\right\\|_2^2 \\\\\n",
    "&=& \\frac{1}{2n} \\left(\\textbf{y} - \\textbf{X} \\textbf{w}\\right)^{\\text{T}} \\left(\\textbf{y} - \\textbf{X} \\textbf{w}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Для розв'язання даної оптимізаційної задачі необхідно обчислити похідні за параметрами моделі, прирівняти їх до нуля і розв'язати отримані рівняння щодо $\\textbf w$ (матричне диференціювання непідготовленому читачеві може здатися складним, спробуйте розписати все через суми, щоб переконатися у відповіді).\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary>Шпаргалка по матричних похідних (натисніть трикутник, щоб розгорнути)</summary>\n",
    "<p>\n",
    "$$\\large \\begin{array}{rcl} \n",
    "\\frac{\\partial}{\\partial \\textbf{X}} \\textbf{X}^{\\text{T}} \\textbf{A} &=& \\textbf{A} \\\\\n",
    "\\frac{\\partial}{\\partial \\textbf{X}} \\textbf{X}^{\\text{T}} \\textbf{A} \\textbf{X} &=& \\left(\\textbf{A} + \\textbf{A}^{\\text{T}}\\right)\\textbf{X} \\\\\n",
    "\\frac{\\partial}{\\partial \\textbf{A}} \\textbf{X}^{\\text{T}} \\textbf{A} \\textbf{y} &=&  \\textbf{X}^{\\text{T}} \\textbf{y}\\\\\n",
    "\\frac{\\partial}{\\partial \\textbf{X}} \\textbf{A}^{-1} &=& -\\textbf{A}^{-1} \\frac{\\partial \\textbf{A}}{\\partial \\textbf{X}} \\textbf{A}^{-1} \n",
    "\\end{array}$$\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "Що ми отримуємо:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \\frac{\\partial L}{\\partial \\textbf{w}} &=& \\frac{\\partial}{\\partial \\textbf{w}} \\frac{1}{2n} \\left( \\textbf{y}^{\\text{T}} \\textbf{y} -2\\textbf{y}^{\\text{T}} \\textbf{X} \\textbf{w} + \\textbf{w}^{\\text{T}} \\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w}\\right) \\\\\n",
    "&=& \\frac{1}{2n} \\left(-2 \\textbf{X}^{\\text{T}} \\textbf{y} + 2\\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \\frac{\\partial L}{\\partial \\textbf{w}} = 0 &\\Leftrightarrow& \\frac{1}{2n} \\left(-2 \\textbf{X}^{\\text{T}} \\textbf{y} + 2\\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w}\\right) = 0 \\\\\n",
    "&\\Leftrightarrow& -\\textbf{X}^{\\text{T}} \\textbf{y} + \\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w} = 0 \\\\\n",
    "&\\Leftrightarrow& \\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w} = \\textbf{X}^{\\text{T}} \\textbf{y} \\\\\n",
    "&\\Leftrightarrow& \\textbf{w} = \\left(\\textbf{X}^{\\text{T}} \\textbf{X}\\right)^{-1} \\textbf{X}^{\\text{T}} \\textbf{y}\n",
    "\\end{array}$$\n",
    "\n",
    "Отже, враховуючи всі означення та умови описані вище, можемо стверджувати, спираючись на [теорему Гаусса-Маркова](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem), що оцінка МНК є найкращою оцінкою параметрів моделі, серед всіх *лінійних* та *незміщених* оцінок, тобто має найменшу дисперсію."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "## 2. Метод максимальної правдоподібності\n",
    "\n",
    "З огляду на проведені вище міркування, цілком природнього може виникнути запитання: наприклад, чому ми мінімізуємо середньоквадратичну помилку, а не щось інше. Адже можна мінімізувати середнє абсолютне значення нев'язки або ще щось. Єдине, що відбудеться в разі такої зміни, так це те, що ми вийдемо з умов теореми Гаусса-Маркова і наші оцінки перестануть бути найкращими серед лінійних і незміщених.\n",
    "\n",
    "Спробуємо з'ясувати, що лежить за середньоквадратичною помилкою. Для цього нам доведеться подивитися на лінійну регресію з ймовірнісної точки зору. Модель, природно, залишається такою ж:\n",
    "\n",
    "$$\\large \\textbf y = \\textbf X \\textbf w + \\epsilon,$$\n",
    "\n",
    "але будемо тепер вважати, що випадкові помилки беруться з центрованого [нормального розподілу](https://uk.wikipedia.org/wiki/%D0%9D%D0%BE%D1%80%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%B8%D0%B9_%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB):\n",
    "\n",
    "$$\\large \\epsilon_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)$$\n",
    "\n",
    "Перепишемо модель в новому світлі:\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "y_i &=& \\sum\\limits_{j=0}^m w_j X_{ij} + \\epsilon_i \\\\\n",
    "&\\sim& \\sum\\limits_{j=0}^m w_j  X_{ij} + \\mathcal{N}\\left(0, \\sigma^2\\right) \\\\\n",
    "p\\left(\\textbf{y} \\mid \\textbf X; \\textbf{w}\\right) &=& \\mathcal{N}\\left(\\sum\\limits_{j=0}^m w_j X_{ij}, \\sigma^2\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Так як приклади беруться незалежно (помилки некорельовані – одна з умов теореми Гаусса-Маркова), то повна правдоподібність даних буде виглядати як добуток функцій щільності $p\\left(y_i\\right)$. Розглянемо логарифм правдоподібності, що дозволить нам перейти від добутку до суми:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\log p\\left(\\textbf{y} \\mid \\textbf X; \\textbf{w}\\right) &=& \\log \\prod\\limits_{i=1}^n \\mathcal{N}\\left(\\sum\\limits_{j=0}^m w_j X_{ij}, \\sigma^2\\right) \\\\\n",
    "&=& \\sum\\limits_{i=1}^n \\log \\mathcal{N}\\left(\\sum\\limits_{j=0}^m w_j X_{ij}, \\sigma^2\\right) \\\\\n",
    "&=& -\\frac{n}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^n \\left(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i\\right)^2\n",
    "\\end{array}$$\n",
    "\n",
    "Потрібно максимізувати вираз $p\\left(\\textbf{y} \\mid \\textbf X; \\textbf{w}\\right)$ за вектором $\\textbf w$, отримавши при цьому $\\textbf{w}_{\\text{ML}}$. Зверніть увагу, що при максимізації функції за якимось параметром можна викинути всі члени, які не залежать від цього параметра:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\textbf{w}_{\\text{ML}} &=& \\arg \\max\\limits_{\\textbf w} p\\left(\\textbf{y}\\mid \\textbf X; \\textbf{w}\\right) = \\arg \\max\\limits_{\\textbf w} \\log p\\left(\\textbf{y}\\mid \\textbf X; \\textbf{w}\\right)\\\\\n",
    "&=& \\arg \\max\\limits_{\\textbf w} -\\frac{n}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^n \\left(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i\\right)^2 \\\\\n",
    "&=& \\arg \\max\\limits_{\\textbf w} -\\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^n \\left(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i\\right)^2 \\\\\n",
    "&=&  \\arg \\min\\limits_{\\textbf w} L\\left(\\textbf X, \\textbf{y}, \\textbf{w} \\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Отже, бачимо, що максимізація правдоподібності даних – це те ж саме, що і мінімізація середньоквадратичної помилки (при справедливості зазначених вище припущень). Виходить, що саме така функція втрат є наслідком того, що помилка розподілена нормально, а не якось по-іншому."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "## 3. Розклад помилки на зсув та розкид (Bias-variance decomposition)\n",
    "\n",
    "Поговоримо трохи про властивості помилки прогнозу лінійної регресії (насправді, наступні міркування справедливі для всіх алгоритмів машинного навчання). Ми з'ясували, що істинне значення цільової змінної складається з деякої детермінованою функції $f(\\textbf{x})$ і випадкової помилки $\\epsilon$: \n",
    "$$\n",
    "\\large y = f\\left(\\textbf{x}\\right) + \\epsilon\n",
    "$$\n",
    "Вважатимемо, що виконуються умови Гаусса-Маркова. Ми намагаємося наблизити детерміновану, але невідому функцію $f\\left(\\textbf{x}\\right)$ лінійною функцією від регресорів $\\widehat{f}\\left(\\textbf{x}\\right)$, яка, в свою чергу, є точковою оцінкою функції $f$ в просторі функцій (точніше, ми обмежили простір функцій параметричним сімейством лінійних функцій), тобто випадковою змінною, у якої є математичне сподівання і дисперсія.\n",
    "\n",
    "Тоді помилка в точці $\\textbf{x}$ розкладається наступним чином:\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "\\text{Err}\\left(\\textbf{x}\\right) &=& \\mathbb{E}\\left[\\left(y - \\widehat{f}\\left(\\textbf{x}\\right)\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[y^2\\right] + \\mathbb{E}\\left[\\left(\\widehat{f}\\left(\\textbf{x}\\right)\\right)^2\\right] - 2\\mathbb{E}\\left[y\\widehat{f}\\left(\\textbf{x}\\right)\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[y^2\\right] + \\mathbb{E}\\left[\\widehat{f}^2\\right] - 2\\mathbb{E}\\left[y\\widehat{f}\\right] \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Для наочності опустимо позначення аргументу функцій. Розглянемо кожен член окремо, перші два розписуються легко за формулою $\\text{Var}\\left(z\\right) = \\mathbb{E}\\left[z^2\\right] - \\mathbb{E}\\left[z\\right]^2$:\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "\\mathbb{E}\\left[y^2\\right] &=& \\text{Var}\\left(y\\right) + \\mathbb{E}\\left[y\\right]^2 = \\sigma^2 + f^2\\\\\n",
    "\\mathbb{E}\\left[\\widehat{f}^2\\right] &=& \\text{Var}\\left(\\widehat{f}\\right) + \\mathbb{E}\\left[\\widehat{f}\\right]^2 \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Справді:\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "\\text{Var}\\left(y\\right) &=& \\mathbb{E}\\left[\\left(y - \\mathbb{E}\\left[y\\right]\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[\\left(y - f\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[\\left(f + \\epsilon - f\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[\\epsilon^2\\right] = \\sigma^2\n",
    "\\end{array}$$\n",
    "\n",
    "$$\\large \\mathbb{E}[y] = \\mathbb{E}[f + \\epsilon] = \\mathbb{E}[f] + \\mathbb{E}[\\epsilon] = f$$\n",
    "\n",
    "І тепер останній член суми. Ми пам'ятаємо, що помилка і цільова змінна незалежні один від одного:\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "\\mathbb{E}\\left[y\\widehat{f}\\right] &=& \\mathbb{E}\\left[\\left(f + \\epsilon\\right)\\widehat{f}\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[f\\widehat{f}\\right] + \\mathbb{E}\\left[\\epsilon\\widehat{f}\\right] \\\\\n",
    "&=& f\\mathbb{E}\\left[\\widehat{f}\\right] + \\mathbb{E}\\left[\\epsilon\\right] \\mathbb{E}\\left[\\widehat{f}\\right]  = f\\mathbb{E}\\left[\\widehat{f}\\right]\n",
    "\\end{array}$$\n",
    "\n",
    "Нарешті, збираємо все разом:\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "\\text{Err}\\left(\\textbf{x}\\right) &=& \\mathbb{E}\\left[\\left(y - \\widehat{f}\\left(\\textbf{x}\\right)\\right)^2\\right] \\\\\n",
    "&=& \\sigma^2 + f^2 + \\text{Var}\\left(\\widehat{f}\\right) + \\mathbb{E}\\left[\\widehat{f}\\right]^2 - 2f\\mathbb{E}\\left[\\widehat{f}\\right] \\\\\n",
    "&=& \\left(f - \\mathbb{E}\\left[\\widehat{f}\\right]\\right)^2 + \\text{Var}\\left(\\widehat{f}\\right) + \\sigma^2 \\\\\n",
    "&=& \\text{Bias}\\left(\\widehat{f}\\right)^2 + \\text{Var}\\left(\\widehat{f}\\right) + \\sigma^2\n",
    "\\end{array}$$\n",
    "\n",
    "Остання формула говорить нам, що помилка прогнозу будь-якої моделі виду $y = f\\left(\\textbf{x}\\right) + \\epsilon$ складається з:\n",
    "\n",
    "- квадрату зсуву: $\\text{Bias}\\left(\\widehat{f}\\right)$ – середня помилка по всеможливих наборах даних;\n",
    "- дисперсії: $\\text{Var}\\left(\\widehat{f}\\right)$ – варіативність помилки, те, на скільки помилка буде відрізнятися, якщо навчати модель на різних наборах даних;\n",
    "- неусувної помилки: $\\sigma^2$.\n",
    "\n",
    "Якщо з останньою ми нічого зробити не можемо, то на перші два доданків ми можемо якось впливати. В ідеалі, звичайно ж, хотілося б мінімізувати обидва ці доданки (лівий верхній квадрат рисунку), але на практиці часто доводиться балансувати між зсувом і нестабільними оцінками (висока дисперсія).\n",
    "\n",
    "<img src=\"../img/bvtf.png\" width=\"480\">\n",
    "\n",
    "Як правило, при збільшенні складності моделі (наприклад, при збільшенні кількості вільних параметрів) збільшується дисперсія (розкид) оцінки, але зменшується зсув. Через те, що навчальний набір даних повністю запам'ятовується замість узагальнення, невеликі зміни призводять до несподіваних результатів (перенавчання). Якщо ж модель слабка, то вона не в змозі вивчити закономірність, в результаті вивчається щось інше, зміщене щодо правильного рішення.\n",
    "\n",
    "<img src=\"../img/biasvariance.png\" width=\"480\">\n",
    "\n",
    "Теорема Гаусса-Маркова якраз стверджує, що МНК-оцінка параметрів лінійної моделі є найкращою в класі незміщених лінійних оцінок, тобто з найменшою дисперсією. Це означає, що якщо існує будь-яка інша незміщена модель $g$ теж з класу лінійних моделей, то ми можемо бути впевнені, що $Var\\left(\\widehat{f}\\right) \\leq Var\\left(g\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "uk"
   },
   "source": [
    "## 4. Регуляризація лінійної регресії\n",
    "\n",
    "Іноді бувають ситуації, коли ми навмисно збільшуємо зсув моделі заради її стабільності, тобто заради зменшення дисперсії моделі $\\text{Var}\\left(\\widehat{f}\\right)$. Однією з умов теореми Гаусса-Маркова є повний ранг матриці за стовпцями $\\textbf{X}$. В іншому випадку розв'язок МНК $\\textbf{w} = \\left(\\textbf{X}^{\\text{T}} \\textbf{X}\\right)^{-1} \\textbf{X}^{\\text{T}} \\textbf{y}$ не існує, так як не існуватиме обернена матриця $\\left(\\textbf{X}^{\\text{T}} \\textbf{X}\\right)^{-1}.$ Іншими словами, матриця $\\textbf{X}^{\\text{T}} \\textbf{X}$ буде виродженою. Така задача називається некоректно сформульованою. Задачу потрібно скорегувати, а саме, зробити матрицю $\\textbf{X}^{\\text{T}}\\textbf{X}$ невиродженою, або регулярною (саме тому цей процес називається регуляризацією). Частіше в даних ми можемо спостерігати так звану *мультиколінеарність* — коли дві або декілька ознак сильно корельовані, в матриці $\\textbf{X}$ це проявляється у вигляді \"майже\" лінійної залежності стовпців. Наприклад, в задачі прогнозування ціни квартири за її параметрами \"майже\" лінійна залежність буде у ознак \"площа з урахуванням балкона\" і \"площа без урахування балкона\". Формально для таких даних матриця $\\textbf{X}^{\\text{T}} \\textbf{X}$ буде оборотна, але через мультиколінеарність у матриці $\\textbf{X}^{\\text{T}} \\textbf{X}$ деякі власні значення будуть близькі до нуля, а в оберненій матриці $\\left(\\textbf{X}^{\\text{T}} \\textbf{X}\\right)^{-1}$ з'являться екстремально великі власні значення, тому що власні значення оберненої матриці – це $\\displaystyle\\frac{1}{\\lambda_i}$. Результатом цього стане нестабільна оцінка параметрів моделі, тобто додавання нового спостереження в набір навчальних даних призведе до зовсім іншого розв'язку. Одним із способів регуляризації є [регуляризація Тихонова](https://uk.wikipedia.org/wiki/%D0%A0%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D1%96%D1%8F_(%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0)#%D0%A0%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D1%96%D1%8F_%D0%A2%D0%B8%D1%85%D0%BE%D0%BD%D0%BE%D0%B2%D0%B0) (L2-регуляризація), яка в загальному полягає у додаванні нового члена до середньоквадратичної помилки:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "L\\left(\\textbf{X}, \\textbf{y}, \\textbf{w} \\right) &=& \\frac{1}{2n} \\left\\| \\textbf{y} - \\textbf{X} \\textbf{w} \\right\\|_2^2 + \\left\\|\\Gamma \\textbf{w}\\right\\|^2\\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Часто матриця Тихонова виражається як добуток деякого числа на одиничну матрицю: $\\Gamma = \\displaystyle\\frac{\\lambda}{2}\\textbf{E}$. У цьому випадку задача мінімізації середньоквадратичної помилки стає задачею з обмеженням на $L_2$-норму. Якщо продиференціювати нову функцію втрат за параметрами моделі, прирівняти отриману функцію до нуля і виразити $\\textbf{w}$, то ми отримаємо точний розв'язок задачі.\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\textbf{w} &=& \\left(\\textbf{X}^{\\text{T}} \\textbf{X} + \\lambda \\textbf{E}\\right)^{-1} \\textbf{X}^{\\text{T}} \\textbf{y}\n",
    "\\end{array}$$\n",
    "\n",
    "Така регресія називається гребеневою регресією (ridge regression). А гребенем є якраз діагональна матриця, яку ми додаємо до матриці $\\textbf{X}^{\\text{T}} \\textbf{X}$, в результаті виходить гарантовано регулярна матриця.\n",
    "\n",
    "<img src=\"../img/ridge.png\">\n",
    "\n",
    "Такий розв'язок зменшує дисперсію, але стає зміщеним, тому що мінімізується також і норма вектора параметрів, що змушує розв'язок зсуватися в сторону нуля. На рисунку нижче на перетині білих пунктирних ліній знаходиться МНК-розв'язок. Блакитними точками позначені різні розв'язки гребневої регресії. Видно, що при збільшенні параметра регуляризації $\\lambda$ розв'язок зсувається в сторону нуля.\n",
    "\n",
    "<img src=\"../img/l2.png\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "ru",
   "targetLang": "uk",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
